// <auto-generated/>

using System;
using System.Collections.Generic;
using System.Linq;

namespace OpenAI.Official.Internal.Models
{
    /// <summary> Model factory for models. </summary>
    internal static partial class OpenAIModelFactory
    {
        /// <summary> Initializes a new instance of <see cref="Models.CreateSpeechRequest"/>. </summary>
        /// <param name="model"> One of the available [TTS models](/docs/models/tts): `tts-1` or `tts-1-hd`. </param>
        /// <param name="input"> The text to generate audio for. The maximum length is 4096 characters. </param>
        /// <param name="voice">
        /// The voice to use when generating the audio. Supported voices are `alloy`, `echo`, `fable`,
        /// `onyx`, `nova`, and `shimmer`. Previews of the voices are available in the
        /// [Text to speech guide](/docs/guides/text-to-speech/voice-options).
        /// </param>
        /// <param name="responseFormat"> The format to audio in. Supported formats are `mp3`, `opus`, `aac`, and `flac`. </param>
        /// <param name="speed"> The speed of the generated audio. Select a value from `0.25` to `4.0`. `1.0` is the default. </param>
        /// <returns> A new <see cref="Models.CreateSpeechRequest"/> instance for mocking. </returns>
        public static CreateSpeechRequest CreateSpeechRequest(CreateSpeechRequestModel model = default, string input = null, CreateSpeechRequestVoice voice = default, CreateSpeechRequestResponseFormat? responseFormat = null, double? speed = null)
        {
            return new CreateSpeechRequest(model, input, voice, responseFormat, speed, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateTranscriptionRequest"/>. </summary>
        /// <param name="file">
        /// The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4,
        /// mpeg, mpga, m4a, ogg, wav, or webm.
        /// </param>
        /// <param name="model"> ID of the model to use. Only `whisper-1` is currently available. </param>
        /// <param name="language">
        /// The language of the input audio. Supplying the input language in
        /// [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy
        /// and latency.
        /// </param>
        /// <param name="prompt">
        /// An optional text to guide the model's style or continue a previous audio segment. The
        /// [prompt](/docs/guides/speech-to-text/prompting) should match the audio language.
        /// </param>
        /// <param name="responseFormat">
        /// The format of the transcript output, in one of these options: json, text, srt, verbose_json, or
        /// vtt.
        /// </param>
        /// <param name="temperature">
        /// The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more
        /// random, while lower values like 0.2 will make it more focused and deterministic. If set to 0,
        /// the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to
        /// automatically increase the temperature until certain thresholds are hit.
        /// </param>
        /// <returns> A new <see cref="Models.CreateTranscriptionRequest"/> instance for mocking. </returns>
        public static CreateTranscriptionRequest CreateTranscriptionRequest(BinaryData file = null, CreateTranscriptionRequestModel model = default, string language = null, string prompt = null, CreateTranscriptionRequestResponseFormat? responseFormat = null, double? temperature = null)
        {
            return new CreateTranscriptionRequest(file, model, language, prompt, responseFormat, temperature, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateTranscriptionResponse"/>. </summary>
        /// <param name="text"> The transcribed text for the provided audio data. </param>
        /// <param name="task"> The label that describes which operation type generated the accompanying response data. </param>
        /// <param name="language"> The spoken language that was detected in the audio data. </param>
        /// <param name="duration"> The total duration of the audio processed to produce accompanying transcription information. </param>
        /// <param name="segments">
        /// A collection of information about the timing, probabilities, and other detail of each processed
        /// audio segment.
        /// </param>
        /// <returns> A new <see cref="Models.CreateTranscriptionResponse"/> instance for mocking. </returns>
        public static CreateTranscriptionResponse CreateTranscriptionResponse(string text = null, CreateTranscriptionResponseTask? task = null, string language = null, TimeSpan? duration = null, IEnumerable<AudioSegment> segments = null)
        {
            segments ??= new List<AudioSegment>();

            return new CreateTranscriptionResponse(text, task, language, duration, segments?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.AudioSegment"/>. </summary>
        /// <param name="id"> The zero-based index of this segment. </param>
        /// <param name="seek">
        /// The seek position associated with the processing of this audio segment. Seek positions are
        /// expressed as hundredths of seconds. The model may process several segments from a single seek
        /// position, so while the seek position will never represent a later time than the segment's
        /// start, the segment's start may represent a significantly later time than the segment's
        /// associated seek position.
        /// </param>
        /// <param name="start"> The time at which this segment started relative to the beginning of the audio. </param>
        /// <param name="end"> The time at which this segment ended relative to the beginning of the audio. </param>
        /// <param name="text"> The text that was part of this audio segment. </param>
        /// <param name="tokens"> The token IDs matching the text in this audio segment. </param>
        /// <param name="temperature"> The temperature score associated with this audio segment. </param>
        /// <param name="avgLogprob"> The average log probability associated with this audio segment. </param>
        /// <param name="compressionRatio"> The compression ratio of this audio segment. </param>
        /// <param name="noSpeechProb"> The probability of no speech detection within this audio segment. </param>
        /// <returns> A new <see cref="Models.AudioSegment"/> instance for mocking. </returns>
        public static AudioSegment AudioSegment(long id = default, long seek = default, TimeSpan start = default, TimeSpan end = default, string text = null, IEnumerable<long> tokens = null, double temperature = default, double avgLogprob = default, double compressionRatio = default, double noSpeechProb = default)
        {
            tokens ??= new List<long>();

            return new AudioSegment(id, seek, start, end, text, tokens?.ToList(), temperature, avgLogprob, compressionRatio, noSpeechProb, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateTranslationRequest"/>. </summary>
        /// <param name="file">
        /// The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4,
        /// mpeg, mpga, m4a, ogg, wav, or webm.
        /// </param>
        /// <param name="model"> ID of the model to use. Only `whisper-1` is currently available. </param>
        /// <param name="prompt">
        /// An optional text to guide the model's style or continue a previous audio segment. The
        /// [prompt](/docs/guides/speech-to-text/prompting) should match the audio language.
        /// </param>
        /// <param name="responseFormat">
        /// The format of the transcript output, in one of these options: json, text, srt, verbose_json, or
        /// vtt.
        /// </param>
        /// <param name="temperature">
        /// The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more
        /// random, while lower values like 0.2 will make it more focused and deterministic. If set to 0,
        /// the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to
        /// automatically increase the temperature until certain thresholds are hit.
        /// </param>
        /// <returns> A new <see cref="Models.CreateTranslationRequest"/> instance for mocking. </returns>
        public static CreateTranslationRequest CreateTranslationRequest(BinaryData file = null, CreateTranslationRequestModel model = default, string prompt = null, CreateTranslationRequestResponseFormat? responseFormat = null, double? temperature = null)
        {
            return new CreateTranslationRequest(file, model, prompt, responseFormat, temperature, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateTranslationResponse"/>. </summary>
        /// <param name="text"> The translated text for the provided audio data. </param>
        /// <param name="task"> The label that describes which operation type generated the accompanying response data. </param>
        /// <param name="language"> The spoken language that was detected in the audio data. </param>
        /// <param name="duration"> The total duration of the audio processed to produce accompanying translation information. </param>
        /// <param name="segments">
        /// A collection of information about the timing, probabilities, and other detail of each processed
        /// audio segment.
        /// </param>
        /// <returns> A new <see cref="Models.CreateTranslationResponse"/> instance for mocking. </returns>
        public static CreateTranslationResponse CreateTranslationResponse(string text = null, CreateTranslationResponseTask? task = null, string language = null, TimeSpan? duration = null, IEnumerable<AudioSegment> segments = null)
        {
            segments ??= new List<AudioSegment>();

            return new CreateTranslationResponse(text, task, language, duration, segments?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateAssistantRequest"/>. </summary>
        /// <param name="model">
        /// ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to
        /// see all of your available models, or see our [Model overview](/docs/models/overview) for
        /// descriptions of them.
        /// </param>
        /// <param name="name"> The name of the assistant. The maximum length is 256 characters. </param>
        /// <param name="description"> The description of the assistant. The maximum length is 512 characters. </param>
        /// <param name="instructions"> The system instructions that the assistant uses. The maximum length is 32768 characters. </param>
        /// <param name="tools">
        /// A list of tool enabled on the assistant. There can be a maximum of 128 tools per assistant.
        /// Tools can be of types `code_interpreter`, `retrieval`, or `function`.
        /// </param>
        /// <param name="fileIds">
        /// A list of [file](/docs/api-reference/files) IDs attached to this assistant. There can be a
        /// maximum of 20 files attached to the assistant. Files are ordered by their creation date in
        /// ascending order.
        /// </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <returns> A new <see cref="Models.CreateAssistantRequest"/> instance for mocking. </returns>
        public static CreateAssistantRequest CreateAssistantRequest(string model = null, string name = null, string description = null, string instructions = null, IEnumerable<BinaryData> tools = null, IEnumerable<string> fileIds = null, IDictionary<string, string> metadata = null)
        {
            tools ??= new List<BinaryData>();
            fileIds ??= new List<string>();
            metadata ??= new Dictionary<string, string>();

            return new CreateAssistantRequest(model, name, description, instructions, tools?.ToList(), fileIds?.ToList(), metadata, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.AssistantObject"/>. </summary>
        /// <param name="id"> The identifier, which can be referenced in API endpoints. </param>
        /// <param name="object"> The object type, which is always `assistant`. </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the assistant was created. </param>
        /// <param name="name"> The name of the assistant. The maximum length is 256 characters. </param>
        /// <param name="description"> The description of the assistant. The maximum length is 512 characters. </param>
        /// <param name="model">
        /// ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to
        /// see all of your available models, or see our [Model overview](/docs/models/overview) for
        /// descriptions of them.
        /// </param>
        /// <param name="instructions"> The system instructions that the assistant uses. The maximum length is 32768 characters. </param>
        /// <param name="tools">
        /// A list of tool enabled on the assistant. There can be a maximum of 128 tools per assistant.
        /// Tools can be of types `code_interpreter`, `retrieval`, or `function`.
        /// </param>
        /// <param name="fileIds">
        /// A list of [file](/docs/api-reference/files) IDs attached to this assistant. There can be a
        /// maximum of 20 files attached to the assistant. Files are ordered by their creation date in
        /// ascending order.
        /// </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <returns> A new <see cref="Models.AssistantObject"/> instance for mocking. </returns>
        public static AssistantObject AssistantObject(string id = null, AssistantObjectObject @object = default, DateTimeOffset createdAt = default, string name = null, string description = null, string model = null, string instructions = null, IEnumerable<BinaryData> tools = null, IEnumerable<string> fileIds = null, IReadOnlyDictionary<string, string> metadata = null)
        {
            tools ??= new List<BinaryData>();
            fileIds ??= new List<string>();
            metadata ??= new Dictionary<string, string>();

            return new AssistantObject(id, @object, createdAt, name, description, model, instructions, tools?.ToList(), fileIds?.ToList(), metadata, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListAssistantsResponse"/>. </summary>
        /// <param name="object"></param>
        /// <param name="data"></param>
        /// <param name="firstId"></param>
        /// <param name="lastId"></param>
        /// <param name="hasMore"></param>
        /// <returns> A new <see cref="Models.ListAssistantsResponse"/> instance for mocking. </returns>
        public static ListAssistantsResponse ListAssistantsResponse(ListAssistantsResponseObject @object = default, IEnumerable<AssistantObject> data = null, string firstId = null, string lastId = null, bool hasMore = default)
        {
            data ??= new List<AssistantObject>();

            return new ListAssistantsResponse(@object, data?.ToList(), firstId, lastId, hasMore, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.DeleteAssistantResponse"/>. </summary>
        /// <param name="id"></param>
        /// <param name="deleted"></param>
        /// <param name="object"></param>
        /// <returns> A new <see cref="Models.DeleteAssistantResponse"/> instance for mocking. </returns>
        public static DeleteAssistantResponse DeleteAssistantResponse(string id = null, bool deleted = default, DeleteAssistantResponseObject @object = default)
        {
            return new DeleteAssistantResponse(id, deleted, @object, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.AssistantFileObject"/>. </summary>
        /// <param name="id"> The identifier, which can be referenced in API endpoints. </param>
        /// <param name="object"> The object type, which is always `assistant.file`. </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the assistant file was created. </param>
        /// <param name="assistantId"> The assistant ID that the file is attached to. </param>
        /// <returns> A new <see cref="Models.AssistantFileObject"/> instance for mocking. </returns>
        public static AssistantFileObject AssistantFileObject(string id = null, AssistantFileObjectObject @object = default, DateTimeOffset createdAt = default, string assistantId = null)
        {
            return new AssistantFileObject(id, @object, createdAt, assistantId, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListAssistantFilesResponse"/>. </summary>
        /// <param name="object"></param>
        /// <param name="data"></param>
        /// <param name="firstId"></param>
        /// <param name="lastId"></param>
        /// <param name="hasMore"></param>
        /// <returns> A new <see cref="Models.ListAssistantFilesResponse"/> instance for mocking. </returns>
        public static ListAssistantFilesResponse ListAssistantFilesResponse(ListAssistantFilesResponseObject @object = default, IEnumerable<AssistantFileObject> data = null, string firstId = null, string lastId = null, bool hasMore = default)
        {
            data ??= new List<AssistantFileObject>();

            return new ListAssistantFilesResponse(@object, data?.ToList(), firstId, lastId, hasMore, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.DeleteAssistantFileResponse"/>. </summary>
        /// <param name="id"></param>
        /// <param name="deleted"></param>
        /// <param name="object"></param>
        /// <returns> A new <see cref="Models.DeleteAssistantFileResponse"/> instance for mocking. </returns>
        public static DeleteAssistantFileResponse DeleteAssistantFileResponse(string id = null, bool deleted = default, DeleteAssistantFileResponseObject @object = default)
        {
            return new DeleteAssistantFileResponse(id, deleted, @object, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateChatCompletionRequest"/>. </summary>
        /// <param name="messages">
        /// A list of messages comprising the conversation so far.
        /// [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
        /// </param>
        /// <param name="model">
        /// ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility)
        /// table for details on which models work with the Chat API.
        /// </param>
        /// <param name="frequencyPenalty">
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
        /// frequency in the text so far, decreasing the model's likelihood to repeat the same line
        /// verbatim.
        ///
        /// [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
        /// </param>
        /// <param name="logitBias">
        /// Modify the likelihood of specified tokens appearing in the completion.
        ///
        /// Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an
        /// associated bias value from -100 to 100. Mathematically, the bias is added to the logits
        /// generated by the model prior to sampling. The exact effect will vary per model, but values
        /// between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
        /// should result in a ban or exclusive selection of the relevant token.
        /// </param>
        /// <param name="logprobs">
        /// Whether to return log probabilities of the output tokens or not. If true, returns the log
        /// probabilities of each output token returned in the `content` of `message`. This option is
        /// currently not available on the `gpt-4-vision-preview` model.
        /// </param>
        /// <param name="topLogprobs">
        /// An integer between 0 and 5 specifying the number of most likely tokens to return at each token
        /// position, each with an associated log probability. `logprobs` must be set to `true` if this
        /// parameter is used.
        /// </param>
        /// <param name="maxTokens">
        /// The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.
        ///
        /// The total length of input tokens and generated tokens is limited by the model's context length.
        /// [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
        /// for counting tokens.
        /// </param>
        /// <param name="n">
        /// How many chat completion choices to generate for each input message. Note that you will be
        /// charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to
        /// minimize costs.
        /// </param>
        /// <param name="presencePenalty">
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
        /// in the text so far, increasing the model's likelihood to talk about new topics.
        ///
        /// [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
        /// </param>
        /// <param name="responseFormat">
        /// An object specifying the format that the model must output. Compatible with
        /// [GPT-4 Turbo](/docs/models/gpt-4-and-gpt-4-turbo) and `gpt-3.5-turbo-1106`.
        ///
        /// Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the
        /// model generates is valid JSON.
        ///
        /// **Important:** when using JSON mode, you **must** also instruct the model to produce JSON
        /// yourself via a system or user message. Without this, the model may generate an unending stream
        /// of whitespace until the generation reaches the token limit, resulting in a long-running and
        /// seemingly "stuck" request. Also note that the message content may be partially cut off if
        /// `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the
        /// conversation exceeded the max context length.
        /// </param>
        /// <param name="seed">
        /// This feature is in Beta.
        ///
        /// If specified, our system will make a best effort to sample deterministically, such that
        /// repeated requests with the same `seed` and parameters should return the same result.
        ///
        /// Determinism is not guaranteed, and you should refer to the `system_fingerprint` response
        /// parameter to monitor changes in the backend.
        /// </param>
        /// <param name="stop"> Up to 4 sequences where the API will stop generating further tokens. </param>
        /// <param name="stream">
        /// If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
        /// [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
        /// as they become available, with the stream terminated by a `data: [DONE]` message.
        /// [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
        /// </param>
        /// <param name="temperature">
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
        /// more random, while lower values like 0.2 will make it more focused and deterministic.
        ///
        /// We generally recommend altering this or `top_p` but not both.
        /// </param>
        /// <param name="topP">
        /// An alternative to sampling with temperature, called nucleus sampling, where the model considers
        /// the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
        /// the top 10% probability mass are considered.
        ///
        /// We generally recommend altering this or `temperature` but not both.
        /// </param>
        /// <param name="tools">
        /// A list of tools the model may call. Currently, only functions are supported as a tool. Use this
        /// to provide a list of functions the model may generate JSON inputs for.
        /// </param>
        /// <param name="toolChoice"></param>
        /// <param name="user">
        /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect
        /// abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
        /// </param>
        /// <param name="functionCall">
        /// Deprecated in favor of `tool_choice`.
        ///
        /// Controls which (if any) function is called by the model. `none` means the model will not call a
        /// function and instead generates a message. `auto` means the model can pick between generating a
        /// message or calling a function. Specifying a particular function via `{"name": "my_function"}`
        /// forces the model to call that function.
        ///
        /// `none` is the default when no functions are present. `auto` is the default if functions are
        /// present.
        /// </param>
        /// <param name="functions">
        /// Deprecated in favor of `tools`.
        ///
        /// A list of functions the model may generate JSON inputs for.
        /// </param>
        /// <returns> A new <see cref="Models.CreateChatCompletionRequest"/> instance for mocking. </returns>
        public static CreateChatCompletionRequest CreateChatCompletionRequest(IEnumerable<BinaryData> messages = null, CreateChatCompletionRequestModel model = default, double? frequencyPenalty = null, IDictionary<string, long> logitBias = null, bool? logprobs = null, long? topLogprobs = null, long? maxTokens = null, long? n = null, double? presencePenalty = null, CreateChatCompletionRequestResponseFormat responseFormat = null, long? seed = null, BinaryData stop = null, bool? stream = null, double? temperature = null, double? topP = null, IEnumerable<ChatCompletionTool> tools = null, BinaryData toolChoice = null, string user = null, BinaryData functionCall = null, IEnumerable<ChatCompletionFunctions> functions = null)
        {
            messages ??= new List<BinaryData>();
            logitBias ??= new Dictionary<string, long>();
            tools ??= new List<ChatCompletionTool>();
            functions ??= new List<ChatCompletionFunctions>();

            return new CreateChatCompletionRequest(messages?.ToList(), model, frequencyPenalty, logitBias, logprobs, topLogprobs, maxTokens, n, presencePenalty, responseFormat, seed, stop, stream, temperature, topP, tools?.ToList(), toolChoice, user, functionCall, functions?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ChatCompletionMessageToolCall"/>. </summary>
        /// <param name="id"> The ID of the tool call. </param>
        /// <param name="type"> The type of the tool. Currently, only `function` is supported. </param>
        /// <param name="function"> The function that the model called. </param>
        /// <returns> A new <see cref="Models.ChatCompletionMessageToolCall"/> instance for mocking. </returns>
        public static ChatCompletionMessageToolCall ChatCompletionMessageToolCall(string id = null, ChatCompletionMessageToolCallType type = default, ChatCompletionMessageToolCallFunction function = null)
        {
            return new ChatCompletionMessageToolCall(id, type, function, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ChatCompletionTool"/>. </summary>
        /// <param name="type"> The type of the tool. Currently, only `function` is supported. </param>
        /// <param name="function"></param>
        /// <returns> A new <see cref="Models.ChatCompletionTool"/> instance for mocking. </returns>
        public static ChatCompletionTool ChatCompletionTool(ChatCompletionToolType type = default, FunctionObject function = null)
        {
            return new ChatCompletionTool(type, function, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ChatCompletionFunctions"/>. </summary>
        /// <param name="description">
        /// A description of what the function does, used by the model to choose when and how to call the
        /// function.
        /// </param>
        /// <param name="name">
        /// The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and
        /// dashes, with a maximum length of 64.
        /// </param>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="Models.ChatCompletionFunctions"/> instance for mocking. </returns>
        public static ChatCompletionFunctions ChatCompletionFunctions(string description = null, string name = null, FunctionParameters parameters = null)
        {
            return new ChatCompletionFunctions(description, name, parameters, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateChatCompletionResponse"/>. </summary>
        /// <param name="id"> A unique identifier for the chat completion. </param>
        /// <param name="choices"> A list of chat completion choices. Can be more than one if `n` is greater than 1. </param>
        /// <param name="created"> The Unix timestamp (in seconds) of when the chat completion was created. </param>
        /// <param name="model"> The model used for the chat completion. </param>
        /// <param name="systemFingerprint">
        /// This fingerprint represents the backend configuration that the model runs with.
        ///
        /// Can be used in conjunction with the `seed` request parameter to understand when backend changes
        /// have been made that might impact determinism.
        /// </param>
        /// <param name="object"> The object type, which is always `chat.completion`. </param>
        /// <param name="usage"></param>
        /// <returns> A new <see cref="Models.CreateChatCompletionResponse"/> instance for mocking. </returns>
        public static CreateChatCompletionResponse CreateChatCompletionResponse(string id = null, IEnumerable<CreateChatCompletionResponseChoice> choices = null, DateTimeOffset created = default, string model = null, string systemFingerprint = null, CreateChatCompletionResponseObject @object = default, CompletionUsage usage = null)
        {
            choices ??= new List<CreateChatCompletionResponseChoice>();

            return new CreateChatCompletionResponse(id, choices?.ToList(), created, model, systemFingerprint, @object, usage, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateChatCompletionResponseChoice"/>. </summary>
        /// <param name="finishReason">
        /// The reason the model stopped generating tokens. This will be `stop` if the model hit a
        /// natural stop point or a provided stop sequence, `length` if the maximum number of tokens
        /// specified in the request was reached, `content_filter` if content was omitted due to a flag
        /// from our content filters, `tool_calls` if the model called a tool, or `function_call`
        /// (deprecated) if the model called a function.
        /// </param>
        /// <param name="index"> The index of the choice in the list of choices. </param>
        /// <param name="message"></param>
        /// <param name="logprobs"> Log probability information for the choice. </param>
        /// <returns> A new <see cref="Models.CreateChatCompletionResponseChoice"/> instance for mocking. </returns>
        public static CreateChatCompletionResponseChoice CreateChatCompletionResponseChoice(CreateChatCompletionResponseChoiceFinishReason finishReason = default, long index = default, ChatCompletionResponseMessage message = null, CreateChatCompletionResponseChoiceLogprobs logprobs = null)
        {
            return new CreateChatCompletionResponseChoice(finishReason, index, message, logprobs, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ChatCompletionResponseMessage"/>. </summary>
        /// <param name="content"> The contents of the message. </param>
        /// <param name="toolCalls"></param>
        /// <param name="role"> The role of the author of this message. </param>
        /// <param name="functionCall"> Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model. </param>
        /// <returns> A new <see cref="Models.ChatCompletionResponseMessage"/> instance for mocking. </returns>
        public static ChatCompletionResponseMessage ChatCompletionResponseMessage(string content = null, IEnumerable<ChatCompletionMessageToolCall> toolCalls = null, ChatCompletionResponseMessageRole role = default, ChatCompletionResponseMessageFunctionCall functionCall = null)
        {
            toolCalls ??= new List<ChatCompletionMessageToolCall>();

            return new ChatCompletionResponseMessage(content, toolCalls?.ToList(), role, functionCall, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ChatCompletionResponseMessageFunctionCall"/>. </summary>
        /// <param name="arguments">
        /// The arguments to call the function with, as generated by the model in JSON format. Note that
        /// the model does not always generate valid JSON, and may hallucinate parameters not defined by
        /// your function schema. Validate the arguments in your code before calling your function.
        /// </param>
        /// <param name="name"> The name of the function to call. </param>
        /// <returns> A new <see cref="Models.ChatCompletionResponseMessageFunctionCall"/> instance for mocking. </returns>
        public static ChatCompletionResponseMessageFunctionCall ChatCompletionResponseMessageFunctionCall(string arguments = null, string name = null)
        {
            return new ChatCompletionResponseMessageFunctionCall(arguments, name, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateChatCompletionResponseChoiceLogprobs"/>. </summary>
        /// <param name="content"></param>
        /// <returns> A new <see cref="Models.CreateChatCompletionResponseChoiceLogprobs"/> instance for mocking. </returns>
        public static CreateChatCompletionResponseChoiceLogprobs CreateChatCompletionResponseChoiceLogprobs(IEnumerable<ChatCompletionTokenLogprob> content = null)
        {
            content ??= new List<ChatCompletionTokenLogprob>();

            return new CreateChatCompletionResponseChoiceLogprobs(content?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ChatCompletionTokenLogprob"/>. </summary>
        /// <param name="token"> The token. </param>
        /// <param name="logprob"> The log probability of this token. </param>
        /// <param name="bytes">
        /// A list of integers representing the UTF-8 bytes representation of the token. Useful in
        /// instances where characters are represented by multiple tokens and their byte representations
        /// must be combined to generate the correct text representation. Can be `null` if there is no
        /// bytes representation for the token.
        /// </param>
        /// <param name="topLogprobs">
        /// List of the most likely tokens and their log probability, at this token position. In rare
        /// cases, there may be fewer than the number of requested `top_logprobs` returned.
        /// </param>
        /// <returns> A new <see cref="Models.ChatCompletionTokenLogprob"/> instance for mocking. </returns>
        public static ChatCompletionTokenLogprob ChatCompletionTokenLogprob(string token = null, double logprob = default, IEnumerable<long> bytes = null, IEnumerable<ChatCompletionTokenLogprobTopLogprob> topLogprobs = null)
        {
            bytes ??= new List<long>();
            topLogprobs ??= new List<ChatCompletionTokenLogprobTopLogprob>();

            return new ChatCompletionTokenLogprob(token, logprob, bytes?.ToList(), topLogprobs?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ChatCompletionTokenLogprobTopLogprob"/>. </summary>
        /// <param name="token"> The token. </param>
        /// <param name="logprob"> The log probability of this token. </param>
        /// <param name="bytes">
        /// A list of integers representing the UTF-8 bytes representation of the token. Useful in
        /// instances where characters are represented by multiple tokens and their byte representations
        /// must be combined to generate the correct text representation. Can be `null` if there is no
        /// bytes representation for the token.
        /// </param>
        /// <returns> A new <see cref="Models.ChatCompletionTokenLogprobTopLogprob"/> instance for mocking. </returns>
        public static ChatCompletionTokenLogprobTopLogprob ChatCompletionTokenLogprobTopLogprob(string token = null, double logprob = default, IEnumerable<long> bytes = null)
        {
            bytes ??= new List<long>();

            return new ChatCompletionTokenLogprobTopLogprob(token, logprob, bytes?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CompletionUsage"/>. </summary>
        /// <param name="promptTokens"> Number of tokens in the prompt. </param>
        /// <param name="completionTokens"> Number of tokens in the generated completion. </param>
        /// <param name="totalTokens"> Total number of tokens used in the request (prompt + completion). </param>
        /// <returns> A new <see cref="Models.CompletionUsage"/> instance for mocking. </returns>
        public static CompletionUsage CompletionUsage(long promptTokens = default, long completionTokens = default, long totalTokens = default)
        {
            return new CompletionUsage(promptTokens, completionTokens, totalTokens, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateCompletionRequest"/>. </summary>
        /// <param name="model">
        /// ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to
        /// see all of your available models, or see our [Model overview](/docs/models/overview) for
        /// descriptions of them.
        /// </param>
        /// <param name="prompt">
        /// The prompt(s) to generate completions for, encoded as a string, array of strings, array of
        /// tokens, or array of token arrays.
        ///
        /// Note that &lt;|endoftext|&gt; is the document separator that the model sees during training, so if a
        /// prompt is not specified the model will generate as if from the beginning of a new document.
        /// </param>
        /// <param name="bestOf">
        /// Generates `best_of` completions server-side and returns the "best" (the one with the highest
        /// log probability per token). ClientResults cannot be streamed.
        ///
        /// When used with `n`, `best_of` controls the number of candidate completions and `n` specifies
        /// how many to return – `best_of` must be greater than `n`.
        ///
        /// **Note:** Because this parameter generates many completions, it can quickly consume your token
        /// quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
        /// </param>
        /// <param name="echo"> Echo back the prompt in addition to the completion. </param>
        /// <param name="frequencyPenalty">
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
        /// frequency in the text so far, decreasing the model's likelihood to repeat the same line
        /// verbatim.
        ///
        /// [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
        /// </param>
        /// <param name="logitBias">
        /// Modify the likelihood of specified tokens appearing in the completion.
        ///
        /// Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an
        /// associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe)
        /// to convert text to token IDs. Mathematically, the bias is added to the logits generated by the
        /// model prior to sampling. The exact effect will vary per model, but values between -1 and 1
        /// should decrease or increase likelihood of selection; values like -100 or 100 should result in a
        /// ban or exclusive selection of the relevant token.
        ///
        /// As an example, you can pass `{"50256": -100}` to prevent the &lt;|endoftext|&gt; token from being
        /// generated.
        /// </param>
        /// <param name="logprobs">
        /// Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens.
        /// For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The
        /// API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1`
        /// elements in the response.
        ///
        /// The maximum value for `logprobs` is 5.
        /// </param>
        /// <param name="maxTokens">
        /// The maximum number of [tokens](/tokenizer) to generate in the completion.
        ///
        /// The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        /// [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
        /// for counting tokens.
        /// </param>
        /// <param name="n">
        /// How many completions to generate for each prompt.
        ///
        /// **Note:** Because this parameter generates many completions, it can quickly consume your token
        /// quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
        /// </param>
        /// <param name="presencePenalty">
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
        /// in the text so far, increasing the model's likelihood to talk about new topics.
        ///
        /// [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
        /// </param>
        /// <param name="seed">
        /// If specified, our system will make a best effort to sample deterministically, such that
        /// repeated requests with the same `seed` and parameters should return the same result.
        ///
        /// Determinism is not guaranteed, and you should refer to the `system_fingerprint` response
        /// parameter to monitor changes in the backend.
        /// </param>
        /// <param name="stop"> Up to 4 sequences where the API will stop generating further tokens. </param>
        /// <param name="stream">
        /// If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
        /// [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
        /// as they become available, with the stream terminated by a `data: [DONE]` message.
        /// [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
        /// </param>
        /// <param name="suffix"> The suffix that comes after a completion of inserted text. </param>
        /// <param name="temperature">
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
        /// more random, while lower values like 0.2 will make it more focused and deterministic.
        ///
        /// We generally recommend altering this or `top_p` but not both.
        /// </param>
        /// <param name="topP">
        /// An alternative to sampling with temperature, called nucleus sampling, where the model considers
        /// the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
        /// the top 10% probability mass are considered.
        ///
        /// We generally recommend altering this or `temperature` but not both.
        /// </param>
        /// <param name="user">
        /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect
        /// abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
        /// </param>
        /// <returns> A new <see cref="Models.CreateCompletionRequest"/> instance for mocking. </returns>
        public static CreateCompletionRequest CreateCompletionRequest(CreateCompletionRequestModel model = default, BinaryData prompt = null, long? bestOf = null, bool? echo = null, double? frequencyPenalty = null, IDictionary<string, long> logitBias = null, long? logprobs = null, long? maxTokens = null, long? n = null, double? presencePenalty = null, long? seed = null, BinaryData stop = null, bool? stream = null, string suffix = null, double? temperature = null, double? topP = null, string user = null)
        {
            logitBias ??= new Dictionary<string, long>();

            return new CreateCompletionRequest(model, prompt, bestOf, echo, frequencyPenalty, logitBias, logprobs, maxTokens, n, presencePenalty, seed, stop, stream, suffix, temperature, topP, user, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateCompletionResponse"/>. </summary>
        /// <param name="id"> A unique identifier for the completion. </param>
        /// <param name="choices"> The list of completion choices the model generated for the input. </param>
        /// <param name="created"> The Unix timestamp (in seconds) of when the completion was created. </param>
        /// <param name="model"> The model used for the completion. </param>
        /// <param name="systemFingerprint">
        /// This fingerprint represents the backend configuration that the model runs with.
        ///
        /// Can be used in conjunction with the `seed` request parameter to understand when backend changes
        /// have been made that might impact determinism.
        /// </param>
        /// <param name="object"> The object type, which is always `text_completion`. </param>
        /// <param name="usage"> Usage statistics for the completion request. </param>
        /// <returns> A new <see cref="Models.CreateCompletionResponse"/> instance for mocking. </returns>
        public static CreateCompletionResponse CreateCompletionResponse(string id = null, IEnumerable<CreateCompletionResponseChoice> choices = null, DateTimeOffset created = default, string model = null, string systemFingerprint = null, CreateCompletionResponseObject @object = default, CompletionUsage usage = null)
        {
            choices ??= new List<CreateCompletionResponseChoice>();

            return new CreateCompletionResponse(id, choices?.ToList(), created, model, systemFingerprint, @object, usage, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateCompletionResponseChoice"/>. </summary>
        /// <param name="index"></param>
        /// <param name="text"></param>
        /// <param name="logprobs"></param>
        /// <param name="finishReason">
        /// The reason the model stopped generating tokens. This will be `stop` if the model hit a
        /// natural stop point or a provided stop sequence, or `content_filter` if content was omitted
        /// due to a flag from our content filters, `length` if the maximum number of tokens specified
        /// in the request was reached, or `content_filter` if content was omitted due to a flag from our
        /// content filters.
        /// </param>
        /// <returns> A new <see cref="Models.CreateCompletionResponseChoice"/> instance for mocking. </returns>
        public static CreateCompletionResponseChoice CreateCompletionResponseChoice(long index = default, string text = null, CreateCompletionResponseChoiceLogprobs logprobs = null, CreateCompletionResponseChoiceFinishReason finishReason = default)
        {
            return new CreateCompletionResponseChoice(index, text, logprobs, finishReason, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateCompletionResponseChoiceLogprobs"/>. </summary>
        /// <param name="tokens"></param>
        /// <param name="tokenLogprobs"></param>
        /// <param name="topLogprobs"></param>
        /// <param name="textOffset"></param>
        /// <returns> A new <see cref="Models.CreateCompletionResponseChoiceLogprobs"/> instance for mocking. </returns>
        public static CreateCompletionResponseChoiceLogprobs CreateCompletionResponseChoiceLogprobs(IEnumerable<string> tokens = null, IEnumerable<double> tokenLogprobs = null, IEnumerable<IDictionary<string, long>> topLogprobs = null, IEnumerable<long> textOffset = null)
        {
            tokens ??= new List<string>();
            tokenLogprobs ??= new List<double>();
            topLogprobs ??= new List<IDictionary<string, long>>();
            textOffset ??= new List<long>();

            return new CreateCompletionResponseChoiceLogprobs(tokens?.ToList(), tokenLogprobs?.ToList(), topLogprobs?.ToList(), textOffset?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.EmbeddingCollection"/>. </summary>
        /// <param name="data"> The list of embeddings generated by the model. </param>
        /// <param name="model"> The name of the model used to generate the embedding. </param>
        /// <param name="object"> The object type, which is always "list". </param>
        /// <param name="usage"> The usage information for the request. </param>
        /// <returns> A new <see cref="Models.EmbeddingCollection"/> instance for mocking. </returns>
        public static EmbeddingCollection EmbeddingCollection(IEnumerable<Embedding> data = null, string model = null, EmbeddingCollectionObject @object = default, EmbeddingTokenUsage usage = null)
        {
            data ??= new List<Embedding>();

            return new EmbeddingCollection(data?.ToList(), model, @object, usage, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.EmbeddingTokenUsage"/>. </summary>
        /// <param name="promptTokens"> The number of tokens used by the prompt. </param>
        /// <param name="totalTokens"> The total number of tokens used by the request. </param>
        /// <returns> A new <see cref="Models.EmbeddingTokenUsage"/> instance for mocking. </returns>
        public static EmbeddingTokenUsage EmbeddingTokenUsage(long promptTokens = default, long totalTokens = default)
        {
            return new EmbeddingTokenUsage(promptTokens, totalTokens, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.OpenAIFile"/>. </summary>
        /// <param name="id"> The file identifier, which can be referenced in the API endpoints. </param>
        /// <param name="bytes"> The size of the file, in bytes. </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the file was created. </param>
        /// <param name="filename"> The name of the file. </param>
        /// <param name="object"> The object type, which is always "file". </param>
        /// <param name="purpose">
        /// The intended purpose of the file. Supported values are `fine-tune`, `fine-tune-results`,
        /// `assistants`, and `assistants_output`.
        /// </param>
        /// <param name="status">
        /// Deprecated. The current status of the file, which can be either `uploaded`, `processed`, or
        /// `error`.
        /// </param>
        /// <param name="statusDetails">
        /// Deprecated. For details on why a fine-tuning training file failed validation, see the `error`
        /// field on `fine_tuning.job`.
        /// </param>
        /// <returns> A new <see cref="Models.OpenAIFile"/> instance for mocking. </returns>
        public static OpenAIFile OpenAIFile(string id = null, long bytes = default, DateTimeOffset createdAt = default, string filename = null, OpenAIFileObject @object = default, OpenAIFilePurpose purpose = default, OpenAIFileStatus status = default, string statusDetails = null)
        {
            return new OpenAIFile(id, bytes, createdAt, filename, @object, purpose, status, statusDetails, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListFilesResponse"/>. </summary>
        /// <param name="data"></param>
        /// <param name="object"></param>
        /// <returns> A new <see cref="Models.ListFilesResponse"/> instance for mocking. </returns>
        public static ListFilesResponse ListFilesResponse(IEnumerable<OpenAIFile> data = null, ListFilesResponseObject @object = default)
        {
            data ??= new List<OpenAIFile>();

            return new ListFilesResponse(data?.ToList(), @object, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.DeleteFileResponse"/>. </summary>
        /// <param name="id"></param>
        /// <param name="object"></param>
        /// <param name="deleted"></param>
        /// <returns> A new <see cref="Models.DeleteFileResponse"/> instance for mocking. </returns>
        public static DeleteFileResponse DeleteFileResponse(string id = null, DeleteFileResponseObject @object = default, bool deleted = default)
        {
            return new DeleteFileResponse(id, @object, deleted, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateFineTuningJobRequest"/>. </summary>
        /// <param name="trainingFile">
        /// The ID of an uploaded file that contains training data.
        ///
        /// See [upload file](/docs/api-reference/files/upload) for how to upload a file.
        ///
        /// Your dataset must be formatted as a JSONL file. Additionally, you must upload your file with
        /// the purpose `fine-tune`.
        ///
        /// See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.
        /// </param>
        /// <param name="validationFile">
        /// The ID of an uploaded file that contains validation data.
        ///
        /// If you provide this file, the data is used to generate validation metrics periodically during
        /// fine-tuning. These metrics can be viewed in the fine-tuning results file. The same data should
        /// not be present in both train and validation files.
        ///
        /// Your dataset must be formatted as a JSONL file. You must upload your file with the purpose
        /// `fine-tune`.
        ///
        /// See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.
        /// </param>
        /// <param name="model">
        /// The name of the model to fine-tune. You can select one of the
        /// [supported models](/docs/guides/fine-tuning/what-models-can-be-fine-tuned).
        /// </param>
        /// <param name="hyperparameters"> The hyperparameters used for the fine-tuning job. </param>
        /// <param name="suffix">
        /// A string of up to 18 characters that will be added to your fine-tuned model name.
        ///
        /// For example, a `suffix` of "custom-model-name" would produce a model name like
        /// `ft:gpt-3.5-turbo:openai:custom-model-name:7p4lURel`.
        /// </param>
        /// <returns> A new <see cref="Models.CreateFineTuningJobRequest"/> instance for mocking. </returns>
        public static CreateFineTuningJobRequest CreateFineTuningJobRequest(string trainingFile = null, string validationFile = null, CreateFineTuningJobRequestModel model = default, CreateFineTuningJobRequestHyperparameters hyperparameters = null, string suffix = null)
        {
            return new CreateFineTuningJobRequest(trainingFile, validationFile, model, hyperparameters, suffix, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.FineTuningJob"/>. </summary>
        /// <param name="id"> The object identifier, which can be referenced in the API endpoints. </param>
        /// <param name="object"> The object type, which is always "fine_tuning.job". </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the fine-tuning job was created. </param>
        /// <param name="finishedAt">
        /// The Unix timestamp (in seconds) for when the fine-tuning job was finished. The value will be
        /// null if the fine-tuning job is still running.
        /// </param>
        /// <param name="model"> The base model that is being fine-tuned. </param>
        /// <param name="fineTunedModel">
        /// The name of the fine-tuned model that is being created. The value will be null if the
        /// fine-tuning job is still running.
        /// </param>
        /// <param name="organizationId"> The organization that owns the fine-tuning job. </param>
        /// <param name="status">
        /// The current status of the fine-tuning job, which can be either `created`, `pending`, `running`,
        /// `succeeded`, `failed`, or `cancelled`.
        /// </param>
        /// <param name="hyperparameters">
        /// The hyperparameters used for the fine-tuning job. See the
        /// [fine-tuning guide](/docs/guides/fine-tuning) for more details.
        /// </param>
        /// <param name="trainingFile">
        /// The file ID used for training. You can retrieve the training data with the
        /// [Files API](/docs/api-reference/files/retrieve-contents).
        /// </param>
        /// <param name="validationFile">
        /// The file ID used for validation. You can retrieve the validation results with the
        /// [Files API](/docs/api-reference/files/retrieve-contents).
        /// </param>
        /// <param name="resultFiles">
        /// The compiled results file ID(s) for the fine-tuning job. You can retrieve the results with the
        /// [Files API](/docs/api-reference/files/retrieve-contents).
        /// </param>
        /// <param name="trainedTokens">
        /// The total number of billable tokens processed by this fine tuning job. The value will be null
        /// if the fine-tuning job is still running.
        /// </param>
        /// <param name="error">
        /// For fine-tuning jobs that have `failed`, this will contain more information on the cause of the
        /// failure.
        /// </param>
        /// <returns> A new <see cref="Models.FineTuningJob"/> instance for mocking. </returns>
        public static FineTuningJob FineTuningJob(string id = null, FineTuningJobObject @object = default, DateTimeOffset createdAt = default, DateTimeOffset? finishedAt = null, string model = null, string fineTunedModel = null, string organizationId = null, FineTuningJobStatus status = default, FineTuningJobHyperparameters hyperparameters = null, string trainingFile = null, string validationFile = null, IEnumerable<string> resultFiles = null, long? trainedTokens = null, FineTuningJobError error = null)
        {
            resultFiles ??= new List<string>();

            return new FineTuningJob(id, @object, createdAt, finishedAt, model, fineTunedModel, organizationId, status, hyperparameters, trainingFile, validationFile, resultFiles?.ToList(), trainedTokens, error, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.FineTuningJobHyperparameters"/>. </summary>
        /// <param name="nEpochs">
        /// The number of epochs to train the model for. An epoch refers to one full cycle through the
        /// training dataset.
        ///
        /// "Auto" decides the optimal number of epochs based on the size of the dataset. If setting the
        /// number manually, we support any number between 1 and 50 epochs.
        /// </param>
        /// <returns> A new <see cref="Models.FineTuningJobHyperparameters"/> instance for mocking. </returns>
        public static FineTuningJobHyperparameters FineTuningJobHyperparameters(BinaryData nEpochs = null)
        {
            return new FineTuningJobHyperparameters(nEpochs, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.FineTuningJobError"/>. </summary>
        /// <param name="message"> A human-readable error message. </param>
        /// <param name="code"> A machine-readable error code. </param>
        /// <param name="param">
        /// The parameter that was invalid, usually `training_file` or `validation_file`. This field
        /// will be null if the failure was not parameter-specific.
        /// </param>
        /// <returns> A new <see cref="Models.FineTuningJobError"/> instance for mocking. </returns>
        public static FineTuningJobError FineTuningJobError(string message = null, string code = null, string param = null)
        {
            return new FineTuningJobError(message, code, param, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListPaginatedFineTuningJobsResponse"/>. </summary>
        /// <param name="object"></param>
        /// <param name="data"></param>
        /// <param name="hasMore"></param>
        /// <returns> A new <see cref="Models.ListPaginatedFineTuningJobsResponse"/> instance for mocking. </returns>
        public static ListPaginatedFineTuningJobsResponse ListPaginatedFineTuningJobsResponse(string @object = null, IEnumerable<FineTuningJob> data = null, bool hasMore = default)
        {
            data ??= new List<FineTuningJob>();

            return new ListPaginatedFineTuningJobsResponse(@object, data?.ToList(), hasMore, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListFineTuningJobEventsResponse"/>. </summary>
        /// <param name="object"></param>
        /// <param name="data"></param>
        /// <returns> A new <see cref="Models.ListFineTuningJobEventsResponse"/> instance for mocking. </returns>
        public static ListFineTuningJobEventsResponse ListFineTuningJobEventsResponse(string @object = null, IEnumerable<FineTuningJobEvent> data = null)
        {
            data ??= new List<FineTuningJobEvent>();

            return new ListFineTuningJobEventsResponse(@object, data?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.FineTuningJobEvent"/>. </summary>
        /// <param name="id"></param>
        /// <param name="object"></param>
        /// <param name="createdAt"></param>
        /// <param name="level"></param>
        /// <param name="message"></param>
        /// <returns> A new <see cref="Models.FineTuningJobEvent"/> instance for mocking. </returns>
        public static FineTuningJobEvent FineTuningJobEvent(string id = null, string @object = null, DateTimeOffset createdAt = default, FineTuningJobEventLevel level = default, string message = null)
        {
            return new FineTuningJobEvent(id, @object, createdAt, level, message, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateImageRequest"/>. </summary>
        /// <param name="prompt">
        /// A text description of the desired image(s). The maximum length is 1000 characters for
        /// `dall-e-2` and 4000 characters for `dall-e-3`.
        /// </param>
        /// <param name="model"> The model to use for image generation. </param>
        /// <param name="n">
        /// The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only `n=1` is
        /// supported.
        /// </param>
        /// <param name="quality">
        /// The quality of the image that will be generated. `hd` creates images with finer details and
        /// greater consistency across the image. This param is only supported for `dall-e-3`.
        /// </param>
        /// <param name="responseFormat"> The format in which the generated images are returned. Must be one of `url` or `b64_json`. </param>
        /// <param name="size">
        /// The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024` for
        /// `dall-e-2`. Must be one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3` models.
        /// </param>
        /// <param name="style">
        /// The style of the generated images. Must be one of `vivid` or `natural`. Vivid causes the model
        /// to lean towards generating hyper-real and dramatic images. Natural causes the model to produce
        /// more natural, less hyper-real looking images. This param is only supported for `dall-e-3`.
        /// </param>
        /// <param name="user">
        /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect
        /// abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
        /// </param>
        /// <returns> A new <see cref="Models.CreateImageRequest"/> instance for mocking. </returns>
        public static CreateImageRequest CreateImageRequest(string prompt = null, CreateImageRequestModel? model = null, long? n = null, CreateImageRequestQuality? quality = null, CreateImageRequestResponseFormat? responseFormat = null, CreateImageRequestSize? size = null, CreateImageRequestStyle? style = null, string user = null)
        {
            return new CreateImageRequest(prompt, model, n, quality, responseFormat, size, style, user, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ImagesResponse"/>. </summary>
        /// <param name="created"></param>
        /// <param name="data"></param>
        /// <returns> A new <see cref="Models.ImagesResponse"/> instance for mocking. </returns>
        public static ImagesResponse ImagesResponse(DateTimeOffset created = default, IEnumerable<Image> data = null)
        {
            data ??= new List<Image>();

            return new ImagesResponse(created, data?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.Image"/>. </summary>
        /// <param name="b64Json"> The base64-encoded JSON of the generated image, if `response_format` is `b64_json`. </param>
        /// <param name="url"> The URL of the generated image, if `response_format` is `url` (default). </param>
        /// <param name="revisedPrompt"> The prompt that was used to generate the image, if there was any revision to the prompt. </param>
        /// <returns> A new <see cref="Models.Image"/> instance for mocking. </returns>
        public static Image Image(BinaryData b64Json = null, Uri url = null, string revisedPrompt = null)
        {
            return new Image(b64Json, url, revisedPrompt, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateImageEditRequest"/>. </summary>
        /// <param name="image">
        /// The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask is not
        /// provided, image must have transparency, which will be used as the mask.
        /// </param>
        /// <param name="prompt"> A text description of the desired image(s). The maximum length is 1000 characters. </param>
        /// <param name="mask">
        /// An additional image whose fully transparent areas (e.g. where alpha is zero) indicate where
        /// `image` should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions
        /// as `image`.
        /// </param>
        /// <param name="model"> The model to use for image generation. Only `dall-e-2` is supported at this time. </param>
        /// <param name="n"> The number of images to generate. Must be between 1 and 10. </param>
        /// <param name="size"> The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`. </param>
        /// <param name="responseFormat"> The format in which the generated images are returned. Must be one of `url` or `b64_json`. </param>
        /// <param name="user">
        /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect
        /// abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
        /// </param>
        /// <returns> A new <see cref="Models.CreateImageEditRequest"/> instance for mocking. </returns>
        public static CreateImageEditRequest CreateImageEditRequest(BinaryData image = null, string prompt = null, BinaryData mask = null, CreateImageEditRequestModel? model = null, long? n = null, CreateImageEditRequestSize? size = null, CreateImageEditRequestResponseFormat? responseFormat = null, string user = null)
        {
            return new CreateImageEditRequest(image, prompt, mask, model, n, size, responseFormat, user, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateImageVariationRequest"/>. </summary>
        /// <param name="image">
        /// The image to use as the basis for the variation(s). Must be a valid PNG file, less than 4MB,
        /// and square.
        /// </param>
        /// <param name="model"> The model to use for image generation. Only `dall-e-2` is supported at this time. </param>
        /// <param name="n"> The number of images to generate. Must be between 1 and 10. </param>
        /// <param name="responseFormat"> The format in which the generated images are returned. Must be one of `url` or `b64_json`. </param>
        /// <param name="size"> The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`. </param>
        /// <param name="user">
        /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect
        /// abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
        /// </param>
        /// <returns> A new <see cref="Models.CreateImageVariationRequest"/> instance for mocking. </returns>
        public static CreateImageVariationRequest CreateImageVariationRequest(BinaryData image = null, CreateImageVariationRequestModel? model = null, long? n = null, CreateImageVariationRequestResponseFormat? responseFormat = null, CreateImageVariationRequestSize? size = null, string user = null)
        {
            return new CreateImageVariationRequest(image, model, n, responseFormat, size, user, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateMessageRequest"/>. </summary>
        /// <param name="role"> The role of the entity that is creating the message. Currently only `user` is supported. </param>
        /// <param name="content"> The content of the message. </param>
        /// <param name="fileIds">
        /// A list of [File](/docs/api-reference/files) IDs that the message should use. There can be a
        /// maximum of 10 files attached to a message. Useful for tools like `retrieval` and
        /// `code_interpreter` that can access and use files.
        /// </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <returns> A new <see cref="Models.CreateMessageRequest"/> instance for mocking. </returns>
        public static CreateMessageRequest CreateMessageRequest(CreateMessageRequestRole role = default, string content = null, IEnumerable<string> fileIds = null, IDictionary<string, string> metadata = null)
        {
            fileIds ??= new List<string>();
            metadata ??= new Dictionary<string, string>();

            return new CreateMessageRequest(role, content, fileIds?.ToList(), metadata, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.MessageObject"/>. </summary>
        /// <param name="id"> The identifier, which can be referenced in API endpoints. </param>
        /// <param name="object"> The object type, which is always `thread.message`. </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the message was created. </param>
        /// <param name="threadId"> The [thread](/docs/api-reference/threads) ID that this message belongs to. </param>
        /// <param name="role"> The entity that produced the message. One of `user` or `assistant`. </param>
        /// <param name="content"> The content of the message in array of text and/or images. </param>
        /// <param name="assistantId">
        /// If applicable, the ID of the [assistant](/docs/api-reference/assistants) that authored this
        /// message.
        /// </param>
        /// <param name="runId">
        /// If applicable, the ID of the [run](/docs/api-reference/runs) associated with the authoring of
        /// this message.
        /// </param>
        /// <param name="fileIds">
        /// A list of [file](/docs/api-reference/files) IDs that the assistant should use. Useful for
        /// tools like retrieval and code_interpreter that can access files. A maximum of 10 files can be
        /// attached to a message.
        /// </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <returns> A new <see cref="Models.MessageObject"/> instance for mocking. </returns>
        public static MessageObject MessageObject(string id = null, MessageObjectObject @object = default, DateTimeOffset createdAt = default, string threadId = null, MessageObjectRole role = default, IEnumerable<BinaryData> content = null, string assistantId = null, string runId = null, IEnumerable<string> fileIds = null, IReadOnlyDictionary<string, string> metadata = null)
        {
            content ??= new List<BinaryData>();
            fileIds ??= new List<string>();
            metadata ??= new Dictionary<string, string>();

            return new MessageObject(id, @object, createdAt, threadId, role, content?.ToList(), assistantId, runId, fileIds?.ToList(), metadata, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListMessagesResponse"/>. </summary>
        /// <param name="object"></param>
        /// <param name="data"></param>
        /// <param name="firstId"></param>
        /// <param name="lastId"></param>
        /// <param name="hasMore"></param>
        /// <returns> A new <see cref="Models.ListMessagesResponse"/> instance for mocking. </returns>
        public static ListMessagesResponse ListMessagesResponse(ListMessagesResponseObject @object = default, IEnumerable<MessageObject> data = null, string firstId = null, string lastId = null, bool hasMore = default)
        {
            data ??= new List<MessageObject>();

            return new ListMessagesResponse(@object, data?.ToList(), firstId, lastId, hasMore, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListMessageFilesResponse"/>. </summary>
        /// <param name="object"></param>
        /// <param name="data"></param>
        /// <param name="firstId"></param>
        /// <param name="lastId"></param>
        /// <param name="hasMore"></param>
        /// <returns> A new <see cref="Models.ListMessageFilesResponse"/> instance for mocking. </returns>
        public static ListMessageFilesResponse ListMessageFilesResponse(ListMessageFilesResponseObject @object = default, IEnumerable<MessageFileObject> data = null, string firstId = null, string lastId = null, bool hasMore = default)
        {
            data ??= new List<MessageFileObject>();

            return new ListMessageFilesResponse(@object, data?.ToList(), firstId, lastId, hasMore, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.MessageFileObject"/>. </summary>
        /// <param name="id"> TThe identifier, which can be referenced in API endpoints. </param>
        /// <param name="object"> The object type, which is always `thread.message.file`. </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the message file was created. </param>
        /// <param name="messageId"> The ID of the [message](/docs/api-reference/messages) that the [File](/docs/api-reference/files) is attached to. </param>
        /// <returns> A new <see cref="Models.MessageFileObject"/> instance for mocking. </returns>
        public static MessageFileObject MessageFileObject(string id = null, MessageFileObjectObject @object = default, DateTimeOffset createdAt = default, string messageId = null)
        {
            return new MessageFileObject(id, @object, createdAt, messageId, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListModelsResponse"/>. </summary>
        /// <param name="object"></param>
        /// <param name="data"></param>
        /// <returns> A new <see cref="Models.ListModelsResponse"/> instance for mocking. </returns>
        public static ListModelsResponse ListModelsResponse(ListModelsResponseObject @object = default, IEnumerable<Model> data = null)
        {
            data ??= new List<Model>();

            return new ListModelsResponse(@object, data?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.Model"/>. </summary>
        /// <param name="id"> The model identifier, which can be referenced in the API endpoints. </param>
        /// <param name="created"> The Unix timestamp (in seconds) when the model was created. </param>
        /// <param name="object"> The object type, which is always "model". </param>
        /// <param name="ownedBy"> The organization that owns the model. </param>
        /// <returns> A new <see cref="Models.Model"/> instance for mocking. </returns>
        public static Model Model(string id = null, DateTimeOffset created = default, ModelObject @object = default, string ownedBy = null)
        {
            return new Model(id, created, @object, ownedBy, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.DeleteModelResponse"/>. </summary>
        /// <param name="id"></param>
        /// <param name="deleted"></param>
        /// <param name="object"></param>
        /// <returns> A new <see cref="Models.DeleteModelResponse"/> instance for mocking. </returns>
        public static DeleteModelResponse DeleteModelResponse(string id = null, bool deleted = default, DeleteModelResponseObject @object = default)
        {
            return new DeleteModelResponse(id, deleted, @object, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateModerationRequest"/>. </summary>
        /// <param name="input"> The input text to classify. </param>
        /// <param name="model">
        /// Two content moderations models are available: `text-moderation-stable` and
        /// `text-moderation-latest`. The default is `text-moderation-latest` which will be automatically
        /// upgraded over time. This ensures you are always using our most accurate model. If you use
        /// `text-moderation-stable`, we will provide advanced notice before updating the model. Accuracy
        /// of `text-moderation-stable` may be slightly lower than for `text-moderation-latest`.
        /// </param>
        /// <returns> A new <see cref="Models.CreateModerationRequest"/> instance for mocking. </returns>
        public static CreateModerationRequest CreateModerationRequest(BinaryData input = null, CreateModerationRequestModel? model = null)
        {
            return new CreateModerationRequest(input, model, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateModerationResponse"/>. </summary>
        /// <param name="id"> The unique identifier for the moderation request. </param>
        /// <param name="model"> The model used to generate the moderation results. </param>
        /// <param name="results"> A list of moderation objects. </param>
        /// <returns> A new <see cref="Models.CreateModerationResponse"/> instance for mocking. </returns>
        public static CreateModerationResponse CreateModerationResponse(string id = null, string model = null, IEnumerable<CreateModerationResponseResult> results = null)
        {
            results ??= new List<CreateModerationResponseResult>();

            return new CreateModerationResponse(id, model, results?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateModerationResponseResult"/>. </summary>
        /// <param name="flagged"> Whether the content violates [OpenAI's usage policies](/policies/usage-policies). </param>
        /// <param name="categories"> A list of the categories, and whether they are flagged or not. </param>
        /// <param name="categoryScores"> A list of the categories along with their scores as predicted by model. </param>
        /// <returns> A new <see cref="Models.CreateModerationResponseResult"/> instance for mocking. </returns>
        public static CreateModerationResponseResult CreateModerationResponseResult(bool flagged = default, CreateModerationResponseResultCategories categories = null, CreateModerationResponseResultCategoryScores categoryScores = null)
        {
            return new CreateModerationResponseResult(flagged, categories, categoryScores, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateModerationResponseResultCategories"/>. </summary>
        /// <param name="hate">
        /// Content that expresses, incites, or promotes hate based on race, gender, ethnicity,
        /// religion, nationality, sexual orientation, disability status, or caste. Hateful content
        /// aimed at non-protected groups (e.g., chess players) is harrassment.
        /// </param>
        /// <param name="hateThreatening">
        /// Hateful content that also includes violence or serious harm towards the targeted group
        /// based on race, gender, ethnicity, religion, nationality, sexual orientation, disability
        /// status, or caste.
        /// </param>
        /// <param name="harassment"> Content that expresses, incites, or promotes harassing language towards any target. </param>
        /// <param name="harassmentThreatening"> Harassment content that also includes violence or serious harm towards any target. </param>
        /// <param name="selfHarm">
        /// Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting,
        /// and eating disorders.
        /// </param>
        /// <param name="selfHarmIntent">
        /// Content where the speaker expresses that they are engaging or intend to engage in acts of
        /// self-harm, such as suicide, cutting, and eating disorders.
        /// </param>
        /// <param name="selfHarmInstructions">
        /// Content that encourages performing acts of self-harm, such as suicide, cutting, and eating
        /// disorders, or that gives instructions or advice on how to commit such acts.
        /// </param>
        /// <param name="sexual">
        /// Content meant to arouse sexual excitement, such as the description of sexual activity, or
        /// that promotes sexual services (excluding sex education and wellness).
        /// </param>
        /// <param name="sexualMinors"> Sexual content that includes an individual who is under 18 years old. </param>
        /// <param name="violence"> Content that depicts death, violence, or physical injury. </param>
        /// <param name="violenceGraphic"> Content that depicts death, violence, or physical injury in graphic detail. </param>
        /// <returns> A new <see cref="Models.CreateModerationResponseResultCategories"/> instance for mocking. </returns>
        public static CreateModerationResponseResultCategories CreateModerationResponseResultCategories(bool hate = default, bool hateThreatening = default, bool harassment = default, bool harassmentThreatening = default, bool selfHarm = default, bool selfHarmIntent = default, bool selfHarmInstructions = default, bool sexual = default, bool sexualMinors = default, bool violence = default, bool violenceGraphic = default)
        {
            return new CreateModerationResponseResultCategories(hate, hateThreatening, harassment, harassmentThreatening, selfHarm, selfHarmIntent, selfHarmInstructions, sexual, sexualMinors, violence, violenceGraphic, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateModerationResponseResultCategoryScores"/>. </summary>
        /// <param name="hate"> The score for the category 'hate'. </param>
        /// <param name="hateThreatening"> The score for the category 'hate/threatening'. </param>
        /// <param name="harassment"> The score for the category 'harassment'. </param>
        /// <param name="harassmentThreatening"> The score for the category 'harassment/threatening'. </param>
        /// <param name="selfHarm"> The score for the category 'self-harm'. </param>
        /// <param name="selfHarmIntent"> The score for the category 'self-harm/intent'. </param>
        /// <param name="selfHarmInstructions"> The score for the category 'self-harm/instructive'. </param>
        /// <param name="sexual"> The score for the category 'sexual'. </param>
        /// <param name="sexualMinors"> The score for the category 'sexual/minors'. </param>
        /// <param name="violence"> The score for the category 'violence'. </param>
        /// <param name="violenceGraphic"> The score for the category 'violence/graphic'. </param>
        /// <returns> A new <see cref="Models.CreateModerationResponseResultCategoryScores"/> instance for mocking. </returns>
        public static CreateModerationResponseResultCategoryScores CreateModerationResponseResultCategoryScores(double hate = default, double hateThreatening = default, double harassment = default, double harassmentThreatening = default, double selfHarm = default, double selfHarmIntent = default, double selfHarmInstructions = default, double sexual = default, double sexualMinors = default, double violence = default, double violenceGraphic = default)
        {
            return new CreateModerationResponseResultCategoryScores(hate, hateThreatening, harassment, harassmentThreatening, selfHarm, selfHarmIntent, selfHarmInstructions, sexual, sexualMinors, violence, violenceGraphic, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateThreadAndRunRequest"/>. </summary>
        /// <param name="assistantId"> The ID of the [assistant](/docs/api-reference/assistants) to use to execute this run. </param>
        /// <param name="thread"> If no thread is provided, an empty thread will be created. </param>
        /// <param name="model">
        /// The ID of the [Model](/docs/api-reference/models) to be used to execute this run. If a value is
        /// provided here, it will override the model associated with the assistant. If not, the model
        /// associated with the assistant will be used.
        /// </param>
        /// <param name="instructions">
        /// Override the default system message of the assistant. This is useful for modifying the behavior
        /// on a per-run basis.
        /// </param>
        /// <param name="tools">
        /// Override the tools the assistant can use for this run. This is useful for modifying the
        /// behavior on a per-run basis.
        /// </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <returns> A new <see cref="Models.CreateThreadAndRunRequest"/> instance for mocking. </returns>
        public static CreateThreadAndRunRequest CreateThreadAndRunRequest(string assistantId = null, CreateThreadRequest thread = null, string model = null, string instructions = null, IEnumerable<BinaryData> tools = null, IDictionary<string, string> metadata = null)
        {
            tools ??= new List<BinaryData>();
            metadata ??= new Dictionary<string, string>();

            return new CreateThreadAndRunRequest(assistantId, thread, model, instructions, tools?.ToList(), metadata, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunObject"/>. </summary>
        /// <param name="id"> The identifier, which can be referenced in API endpoints. </param>
        /// <param name="object"> The object type, which is always `thread.run`. </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the run was created. </param>
        /// <param name="threadId">
        /// The ID of the [thread](/docs/api-reference/threads) that was executed on as a part of this
        /// run.
        /// </param>
        /// <param name="assistantId"> The ID of the [assistant](/docs/api-reference/assistants) used for execution of this run. </param>
        /// <param name="status">
        /// The status of the run, which can be either `queued`, `in_progress`, `requires_action`,
        /// `cancelling`, `cancelled`, `failed`, `completed`, or `expired`.
        /// </param>
        /// <param name="requiredAction">
        /// Details on the action required to continue the run. Will be `null` if no action is
        /// required.
        /// </param>
        /// <param name="lastError"> The last error associated with this run. Will be `null` if there are no errors. </param>
        /// <param name="expiresAt"> The Unix timestamp (in seconds) for when the run will expire. </param>
        /// <param name="startedAt"> The Unix timestamp (in seconds) for when the run was started. </param>
        /// <param name="cancelledAt"> The Unix timestamp (in seconds) for when the run was cancelled. </param>
        /// <param name="failedAt"> The Unix timestamp (in seconds) for when the run failed. </param>
        /// <param name="completedAt"> The Unix timestamp (in seconds) for when the run was completed. </param>
        /// <param name="model"> The model that the [assistant](/docs/api-reference/assistants) used for this run. </param>
        /// <param name="instructions"> The instructions that the [assistant](/docs/api-reference/assistants) used for this run. </param>
        /// <param name="tools"> The list of tools that the [assistant](/docs/api-reference/assistants) used for this run. </param>
        /// <param name="fileIds">
        /// The list of [File](/docs/api-reference/files) IDs the
        /// [assistant](/docs/api-reference/assistants) used for this run.
        /// </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <param name="usage"></param>
        /// <returns> A new <see cref="Models.RunObject"/> instance for mocking. </returns>
        public static RunObject RunObject(string id = null, RunObjectObject @object = default, DateTimeOffset createdAt = default, string threadId = null, string assistantId = null, RunObjectStatus status = default, RunObjectRequiredAction requiredAction = null, RunObjectLastError lastError = null, DateTimeOffset expiresAt = default, DateTimeOffset? startedAt = null, DateTimeOffset? cancelledAt = null, DateTimeOffset? failedAt = null, DateTimeOffset? completedAt = null, string model = null, string instructions = null, IEnumerable<BinaryData> tools = null, IEnumerable<string> fileIds = null, IReadOnlyDictionary<string, string> metadata = null, RunCompletionUsage usage = null)
        {
            tools ??= new List<BinaryData>();
            fileIds ??= new List<string>();
            metadata ??= new Dictionary<string, string>();

            return new RunObject(id, @object, createdAt, threadId, assistantId, status, requiredAction, lastError, expiresAt, startedAt, cancelledAt, failedAt, completedAt, model, instructions, tools?.ToList(), fileIds?.ToList(), metadata, usage, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunObjectRequiredAction"/>. </summary>
        /// <param name="type"> For now, this is always `submit_tool_outputs`. </param>
        /// <param name="submitToolOutputs"> Details on the tool outputs needed for this run to continue. </param>
        /// <returns> A new <see cref="Models.RunObjectRequiredAction"/> instance for mocking. </returns>
        public static RunObjectRequiredAction RunObjectRequiredAction(RunObjectRequiredActionType type = default, RunObjectRequiredActionSubmitToolOutputs submitToolOutputs = null)
        {
            return new RunObjectRequiredAction(type, submitToolOutputs, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunObjectRequiredActionSubmitToolOutputs"/>. </summary>
        /// <param name="toolCalls"> A list of the relevant tool calls. </param>
        /// <returns> A new <see cref="Models.RunObjectRequiredActionSubmitToolOutputs"/> instance for mocking. </returns>
        public static RunObjectRequiredActionSubmitToolOutputs RunObjectRequiredActionSubmitToolOutputs(IEnumerable<RunToolCallObject> toolCalls = null)
        {
            toolCalls ??= new List<RunToolCallObject>();

            return new RunObjectRequiredActionSubmitToolOutputs(toolCalls?.ToList(), serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunToolCallObject"/>. </summary>
        /// <param name="id">
        /// The ID of the tool call. This ID must be referenced when you submit the tool outputs in using
        /// the [Submit tool outputs to run](/docs/api-reference/runs/submitToolOutputs) endpoint.
        /// </param>
        /// <param name="type"> The type of tool call the output is required for. For now, this is always `function`. </param>
        /// <param name="function"> The function definition. </param>
        /// <returns> A new <see cref="Models.RunToolCallObject"/> instance for mocking. </returns>
        public static RunToolCallObject RunToolCallObject(string id = null, RunToolCallObjectType type = default, RunToolCallObjectFunction function = null)
        {
            return new RunToolCallObject(id, type, function, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunToolCallObjectFunction"/>. </summary>
        /// <param name="name"> The name of the function. </param>
        /// <param name="arguments"> The arguments that the model expects you to pass to the function. </param>
        /// <returns> A new <see cref="Models.RunToolCallObjectFunction"/> instance for mocking. </returns>
        public static RunToolCallObjectFunction RunToolCallObjectFunction(string name = null, string arguments = null)
        {
            return new RunToolCallObjectFunction(name, arguments, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunObjectLastError"/>. </summary>
        /// <param name="code"> One of `server_error` or `rate_limit_exceeded`. </param>
        /// <param name="message"> A human-readable description of the error. </param>
        /// <returns> A new <see cref="Models.RunObjectLastError"/> instance for mocking. </returns>
        public static RunObjectLastError RunObjectLastError(RunObjectLastErrorCode code = default, string message = null)
        {
            return new RunObjectLastError(code, message, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunCompletionUsage"/>. </summary>
        /// <param name="completionTokens"> Number of completion tokens used over the course of the run. </param>
        /// <param name="promptTokens"> Number of prompt tokens used over the course of the run. </param>
        /// <param name="totalTokens"> Total number of tokens used (prompt + completion). </param>
        /// <returns> A new <see cref="Models.RunCompletionUsage"/> instance for mocking. </returns>
        public static RunCompletionUsage RunCompletionUsage(long completionTokens = default, long promptTokens = default, long totalTokens = default)
        {
            return new RunCompletionUsage(completionTokens, promptTokens, totalTokens, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.CreateRunRequest"/>. </summary>
        /// <param name="assistantId"> The ID of the [assistant](/docs/api-reference/assistants) to use to execute this run. </param>
        /// <param name="model">
        /// The ID of the [Model](/docs/api-reference/models) to be used to execute this run. If a value
        /// is provided here, it will override the model associated with the assistant. If not, the model
        /// associated with the assistant will be used.
        /// </param>
        /// <param name="instructions">
        /// Overrides the [instructions](/docs/api-reference/assistants/createAssistant) of the assistant.
        /// This is useful for modifying the behavior on a per-run basis.
        /// </param>
        /// <param name="additionalInstructions">
        /// Appends additional instructions at the end of the instructions for the run. This is useful for
        /// modifying the behavior on a per-run basis without overriding other instructions.
        /// </param>
        /// <param name="tools">
        /// Override the tools the assistant can use for this run. This is useful for modifying the
        /// behavior on a per-run basis.
        /// </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <returns> A new <see cref="Models.CreateRunRequest"/> instance for mocking. </returns>
        public static CreateRunRequest CreateRunRequest(string assistantId = null, string model = null, string instructions = null, string additionalInstructions = null, IEnumerable<BinaryData> tools = null, IDictionary<string, string> metadata = null)
        {
            tools ??= new List<BinaryData>();
            metadata ??= new Dictionary<string, string>();

            return new CreateRunRequest(assistantId, model, instructions, additionalInstructions, tools?.ToList(), metadata, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListRunsResponse"/>. </summary>
        /// <param name="object"></param>
        /// <param name="data"></param>
        /// <param name="firstId"></param>
        /// <param name="lastId"></param>
        /// <param name="hasMore"></param>
        /// <returns> A new <see cref="Models.ListRunsResponse"/> instance for mocking. </returns>
        public static ListRunsResponse ListRunsResponse(ListRunsResponseObject @object = default, IEnumerable<RunObject> data = null, string firstId = null, string lastId = null, bool hasMore = default)
        {
            data ??= new List<RunObject>();

            return new ListRunsResponse(@object, data?.ToList(), firstId, lastId, hasMore, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ListRunStepsResponse"/>. </summary>
        /// <param name="object"></param>
        /// <param name="data"></param>
        /// <param name="firstId"></param>
        /// <param name="lastId"></param>
        /// <param name="hasMore"></param>
        /// <returns> A new <see cref="Models.ListRunStepsResponse"/> instance for mocking. </returns>
        public static ListRunStepsResponse ListRunStepsResponse(ListRunStepsResponseObject @object = default, IEnumerable<RunStepObject> data = null, string firstId = null, string lastId = null, bool hasMore = default)
        {
            data ??= new List<RunStepObject>();

            return new ListRunStepsResponse(@object, data?.ToList(), firstId, lastId, hasMore, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunStepObject"/>. </summary>
        /// <param name="id"> The identifier of the run step, which can be referenced in API endpoints. </param>
        /// <param name="object"> The object type, which is always `thread.run.step`. </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the run step was created. </param>
        /// <param name="assistantId"> The ID of the [assistant](/docs/api-reference/assistants) associated with the run step. </param>
        /// <param name="threadId"> The ID of the [thread](/docs/api-reference/threads) that was run. </param>
        /// <param name="runId"> The ID of the [run](/docs/api-reference/runs) that this run step is a part of. </param>
        /// <param name="type"> The type of run step, which can be either `message_creation` or `tool_calls`. </param>
        /// <param name="status">
        /// The status of the run step, which can be either `in_progress`, `cancelled`, `failed`,
        /// `completed`, or `expired`.
        /// </param>
        /// <param name="stepDetails"> The details of the run step. </param>
        /// <param name="lastError"> The last error associated with this run step. Will be `null` if there are no errors. </param>
        /// <param name="expiresAt">
        /// The Unix timestamp (in seconds) for when the run step expired. A step is considered expired
        /// if the parent run is expired.
        /// </param>
        /// <param name="cancelledAt"> The Unix timestamp (in seconds) for when the run step was cancelled. </param>
        /// <param name="failedAt"> The Unix timestamp (in seconds) for when the run step failed. </param>
        /// <param name="completedAt"> T The Unix timestamp (in seconds) for when the run step completed.. </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <param name="usage"></param>
        /// <returns> A new <see cref="Models.RunStepObject"/> instance for mocking. </returns>
        public static RunStepObject RunStepObject(string id = null, RunStepObjectObject @object = default, DateTimeOffset createdAt = default, string assistantId = null, string threadId = null, string runId = null, RunStepObjectType type = default, RunStepObjectStatus status = default, BinaryData stepDetails = null, RunStepObjectLastError lastError = null, DateTimeOffset? expiresAt = null, DateTimeOffset? cancelledAt = null, DateTimeOffset? failedAt = null, DateTimeOffset? completedAt = null, IReadOnlyDictionary<string, string> metadata = null, RunCompletionUsage usage = null)
        {
            metadata ??= new Dictionary<string, string>();

            return new RunStepObject(id, @object, createdAt, assistantId, threadId, runId, type, status, stepDetails, lastError, expiresAt, cancelledAt, failedAt, completedAt, metadata, usage, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.RunStepObjectLastError"/>. </summary>
        /// <param name="code"> One of `server_error` or `rate_limit_exceeded`. </param>
        /// <param name="message"> A human-readable description of the error. </param>
        /// <returns> A new <see cref="Models.RunStepObjectLastError"/> instance for mocking. </returns>
        public static RunStepObjectLastError RunStepObjectLastError(RunStepObjectLastErrorCode code = default, string message = null)
        {
            return new RunStepObjectLastError(code, message, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.ThreadObject"/>. </summary>
        /// <param name="id"> The identifier, which can be referenced in API endpoints. </param>
        /// <param name="object"> The object type, which is always `thread`. </param>
        /// <param name="createdAt"> The Unix timestamp (in seconds) for when the thread was created. </param>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing
        /// additional information about the object in a structured format. Keys can be a maximum of 64
        /// characters long and values can be a maxium of 512 characters long.
        /// </param>
        /// <returns> A new <see cref="Models.ThreadObject"/> instance for mocking. </returns>
        public static ThreadObject ThreadObject(string id = null, ThreadObjectObject @object = default, DateTimeOffset createdAt = default, IReadOnlyDictionary<string, string> metadata = null)
        {
            metadata ??= new Dictionary<string, string>();

            return new ThreadObject(id, @object, createdAt, metadata, serializedAdditionalRawData: null);
        }

        /// <summary> Initializes a new instance of <see cref="Models.DeleteThreadResponse"/>. </summary>
        /// <param name="id"></param>
        /// <param name="deleted"></param>
        /// <param name="object"></param>
        /// <returns> A new <see cref="Models.DeleteThreadResponse"/> instance for mocking. </returns>
        public static DeleteThreadResponse DeleteThreadResponse(string id = null, bool deleted = default, DeleteThreadResponseObject @object = default)
        {
            return new DeleteThreadResponse(id, deleted, @object, serializedAdditionalRawData: null);
        }
    }
}

